(1/m)*sum((yhat-y)*1)
}
dJ1 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x1)
}
dJ2 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x2)
}
## perform gradient descent
convg_threshold = 0.0001
alpha = 0.01
#re-scale x1 and x2
admission <- admission |>
mutate(
exam1_rescale = exam1 / max(exam1),
exam2_rescale = exam2 / max(exam2)
)
initial_theta0 = -120
initial_theta1 = 1
initial_theta2 = 1
x1 <- admission$exam1_rescale
x2 <- admission$exam2_rescale
y = admission$admission
m <- nrow(admission)
initial_J = J(initial_theta0, initial_theta1, initial_theta2, m, x1, x2, y)
initial_J
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_theta2 = initial_theta2
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterations
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterations
theta2_values = c(current_theta2) # a vector to store updated theta2 values during iterations
J_values = c(current_J) # a vector to store cost function values during iterations
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) {
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta2 = current_theta2 - alpha*dJ2(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_J = J(new_theta0, new_theta1, new_theta2, m, x1, x2, y)
theta0_values = c(theta0_values, new_theta0)
theta1_values = c(theta1_values, new_theta1)
theta2_values = c(theta2_values, new_theta2)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, theta2, and J values
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
current_theta2 = new_theta2
}
# plot for theta_0
plot(theta0_values,
xlab = "Iteration Number",
ylab = "theta0",
main = "theta0 over time")
# plot for theta_1
plot(theta1_values,
xlab = "Iteration Number",
ylab = "theta1",
main = "theta1 over time")
# plot for theta_2
plot(theta2_values,
xlab = "Iteration Number",
ylab = "theta1",
main = "theta1 over time")
# plot for cost function
plot(J_values,
xlab = "Iteration Number",
ylab = "Cost Function [J(theta0, theta1)]",
main = "Cost Function over time")
require(boot)
## gradient descent ##
# cost function for a given y and hypothesis value #
cost = function(y,yhat) {
if(y==1) cost = -log(yhat)
if(y==0) cost = -log(1-yhat)
cost
}
#total cost#
J = function(theta0, theta1, theta2, m, x1, x2, y){
## inv.logit is the sigmoid function in the "boot" package ##
yhat = inv.logit(theta0 + theta1*x1 + theta2*x2)
sum_cost = (1/m)*sum(mapply(FUN=cost,y,yhat))
sum_cost
}
## define functions to compute the partial derivative terms
dJ0 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*1)
}
dJ1 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x1)
}
dJ2 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x2)
}
## perform gradient descent
convg_threshold = 0.0001
alpha = 0.001
#re-scale x1 and x2
admission <- admission |>
mutate(
exam1_rescale = exam1 / max(exam1),
exam2_rescale = exam2 / max(exam2)
)
initial_theta0 = -120
initial_theta1 = 1
initial_theta2 = 1
x1 <- admission$exam1_rescale
x2 <- admission$exam2_rescale
y = admission$admission
m <- nrow(admission)
initial_J = J(initial_theta0, initial_theta1, initial_theta2, m, x1, x2, y)
initial_J
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_theta2 = initial_theta2
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterations
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterations
theta2_values = c(current_theta2) # a vector to store updated theta2 values during iterations
J_values = c(current_J) # a vector to store cost function values during iterations
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) {
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta2 = current_theta2 - alpha*dJ2(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_J = J(new_theta0, new_theta1, new_theta2, m, x1, x2, y)
theta0_values = c(theta0_values, new_theta0)
theta1_values = c(theta1_values, new_theta1)
theta2_values = c(theta2_values, new_theta2)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, theta2, and J values
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
current_theta2 = new_theta2
}
# plot for theta_0
plot(theta0_values,
xlab = "Iteration Number",
ylab = "theta0",
main = "theta0 over time")
# plot for theta_1
plot(theta1_values,
xlab = "Iteration Number",
ylab = "theta1",
main = "theta1 over time")
# plot for theta_2
plot(theta2_values,
xlab = "Iteration Number",
ylab = "theta1",
main = "theta1 over time")
# plot for cost function
plot(J_values,
xlab = "Iteration Number",
ylab = "Cost Function",
main = "Cost Function over time")
require(boot)
## gradient descent ##
# cost function for a given y and hypothesis value #
cost = function(y,yhat) {
if(y==1) cost = -log(yhat)
if(y==0) cost = -log(1-yhat)
cost
}
#total cost#
J = function(theta0, theta1, theta2, m, x1, x2, y){
## inv.logit is the sigmoid function in the "boot" package ##
yhat = inv.logit(theta0 + theta1*x1 + theta2*x2)
sum_cost = (1/m)*sum(mapply(FUN=cost,y,yhat))
sum_cost
}
## define functions to compute the partial derivative terms
dJ0 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*1)
}
dJ1 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x1)
}
dJ2 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x2)
}
## perform gradient descent
convg_threshold = 0.0001
alpha = 0.01
#re-scale x1 and x2
admission <- admission |>
mutate(
exam1_rescale = exam1 / max(exam1),
exam2_rescale = exam2 / max(exam2)
)
initial_theta0 = -120
initial_theta1 = 1
initial_theta2 = 1
x1 <- admission$exam1_rescale
x2 <- admission$exam2_rescale
y = admission$admission
m <- nrow(admission)
initial_J = J(initial_theta0, initial_theta1, initial_theta2, m, x1, x2, y)
initial_J
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_theta2 = initial_theta2
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterations
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterations
theta2_values = c(current_theta2) # a vector to store updated theta2 values during iterations
J_values = c(current_J) # a vector to store cost function values during iterations
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) {
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta2 = current_theta2 - alpha*dJ2(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_J = J(new_theta0, new_theta1, new_theta2, m, x1, x2, y)
theta0_values = c(theta0_values, new_theta0)
theta1_values = c(theta1_values, new_theta1)
theta2_values = c(theta2_values, new_theta2)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, theta2, and J values
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
current_theta2 = new_theta2
}
# plot for theta_0
plot(theta0_values,
xlab = "Iteration Number",
ylab = "theta0",
main = "theta0 over time")
# plot for theta_1
plot(theta1_values,
xlab = "Iteration Number",
ylab = "theta1",
main = "theta1 over time")
# plot for theta_2
plot(theta2_values,
xlab = "Iteration Number",
ylab = "theta1",
main = "theta1 over time")
# plot for cost function
plot(J_values,
xlab = "Iteration Number",
ylab = "Cost Function",
main = "Cost Function over time")
current_theta0
current_theta1
current_theta2
esquisser()
# Decision boundary parameters (from your logistic regression)
intercept <- -59.45505
coef_x1 <- 45.73143
coef_x2 <- 45.51044
# Create the plot
plot(admission$exam1_rescale, admission$exam2_rescale,
col = ifelse(admission$admission == 1, "blue", "red"),
pch = 19, xlab = "Exam 1 Score", ylab = "Exam 2 Score",
main = "Scores on Exam 1 vs Exam 2")
# Adding the decision boundary using curve()
curve(
(intercept + coef_x1 * x) / -coef_x2,
from = min(admission$exam1_rescale),
to = max(admission$exam1_rescale),
add = TRUE, col = "green", lwd = 2
)
equation_text <- paste("Decision Boundary: x2 = (", intercept, " + ", coef_x1, " * x1) / ", -coef_x2)
text(x = 0, y = 2, labels = equation_text, col = "green", cex = 1.2)
yhat = inv.logit(current_theta0 + current_theta1*x1 + current_theta2*x2)
db = inv.logit(current_theta0 + current_theta1*x1 + current_theta2*x2)
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_line(data = db)
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(db)
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green")
db = function(current_theta0, current_theta1, current_theta2, x1, x2){
db = inv.logit(current_theta0 + current_theta1*x1 + current_theta2*x2)
}
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green")
db = function(current_theta0, current_theta1, current_theta2, x1, x2){
inv.logit(current_theta0 + current_theta1*x1 + current_theta2*x2)
}
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green")
db <- function(x) {
# Parameters from logistic regression model
current_theta0 <- -59.45505
current_theta1 <- 45.73143
current_theta2 <- 45.51044
# Compute the decision boundary (exam2_rescale) as a function of exam1_rescale (x)
return((current_theta0 + current_theta1 * x) / -current_theta2)
}
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green")
require(boot)
## gradient descent ##
# cost function for a given y and hypothesis value #
cost = function(y,yhat) {
if(y==1) cost = -log(yhat)
if(y==0) cost = -log(1-yhat)
cost
}
#total cost#
J = function(theta0, theta1, theta2, m, x1, x2, y){
## inv.logit is the sigmoid function in the "boot" package ##
yhat = inv.logit(theta0 + theta1*x1 + theta2*x2)
sum_cost = (1/m)*sum(mapply(FUN=cost,y,yhat))
sum_cost
}
## define functions to compute the partial derivative terms
dJ0 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*1)
}
dJ1 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x1)
}
dJ2 = function(theta0, theta1, theta2, m, x1, x2, y){
yhat= inv.logit(theta0 + theta1*x1 + theta2*x2)
(1/m)*sum((yhat-y)*x2)
}
## perform gradient descent
convg_threshold = 0.0001
alpha = 0.01
#re-scale x1 and x2
admission <- admission |>
mutate(
exam1_rescale = exam1 / max(exam1),
exam2_rescale = exam2 / max(exam2)
)
initial_theta0 = -120
initial_theta1 = 1
initial_theta2 = 1
x1 <- admission$exam1_rescale
x2 <- admission$exam2_rescale
y = admission$admission
m <- nrow(admission)
initial_J = J(initial_theta0, initial_theta1, initial_theta2, m, x1, x2, y)
initial_J
current_theta0 = initial_theta0
current_theta1 = initial_theta1
current_theta2 = initial_theta2
current_J = initial_J
theta0_values = c(current_theta0) # a vector to store updated theta0 values during iterations
theta1_values = c(current_theta1) # a vector to store updated theta1 values during iterations
theta2_values = c(current_theta2) # a vector to store updated theta2 values during iterations
J_values = c(current_J) # a vector to store cost function values during iterations
delta_J = 999 # an arbitrary value to start the while loop
while (delta_J>convg_threshold) {
new_theta0 = current_theta0 - alpha*dJ0(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta1 = current_theta1 - alpha*dJ1(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_theta2 = current_theta2 - alpha*dJ2(current_theta0, current_theta1, current_theta2, m, x1, x2, y)
new_J = J(new_theta0, new_theta1, new_theta2, m, x1, x2, y)
theta0_values = c(theta0_values, new_theta0)
theta1_values = c(theta1_values, new_theta1)
theta2_values = c(theta2_values, new_theta2)
J_values = c(J_values, new_J)
# absolute difference between new cost function and current cost function value
delta_J = abs(new_J - current_J)
## update current theta0, theta1, theta2, and J values
current_J = new_J
current_theta0 = new_theta0
current_theta1 = new_theta1
current_theta2 = new_theta2
}
library(ggplot2)
db <- function(x) {
# Parameters from logistic regression model
current_theta0
current_theta1
current_theta2
# Compute the decision boundary (exam2_rescale) as a function of exam1_rescale (x)
return((current_theta0 + current_theta1 * x) / -current_theta2)
}
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green")
library(ggplot2)
db <- function(x) {
# Parameters from logistic regression model
current_theta0
current_theta1
current_theta2
# Compute the decision boundary (exam2_rescale) as a function of exam1_rescale (x)
return((current_theta0 + current_theta1 * x) / -current_theta2)
}
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green")
exam1_68 = 68/max(admission$exam1)
exam2_75 = 75/max(admission$exam2)
exam1_68 = 68/max(admission$exam1)
exam2_75 = 75/max(admission$exam2)
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green") +
geom_point(aes(x = exam1_68, y = exam2_75), color = "red", size = 3, shape = 19) +
annotate("text", x = 0.68, y = 0.75, label = "Student Scores", color = "red", size = 5, hjust = -0.1)
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green") +
geom_point(aes(x = exam1_68, y = exam2_75), color = "red", size = 3, shape = 19) +
annotate("text", x = 0.68, y = 0.75, label = "Student's Exam Scores", color = "red", size = 5, hjust = -0.1)
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green") +
geom_point(aes(x = exam1_68, y = exam2_75), color = "red", size = 3, shape = 19) +
annotate("text", x = 0.68, y = 0.75, label = "Student's Exam Scores", color = "red", size = 5)
ggplot(admission) +
aes(x = exam1_rescale, y = exam2_rescale, colour = admission) +
geom_point(size = 1.75) +
scale_color_viridis_c(option = "viridis",
direction = 1) +
labs(x = "Exam 1 Score", y = "Exam 2 Score", title = "Scores on Exam 2 vs Exam 1",
color = "Admission") +
theme_bw() +
geom_function(fun = db, color = "green") +
geom_point(aes(x = exam1_68, y = exam2_75), color = "red", size = 3, shape = 19) +
annotate("text", x = 0.68, y = 0.75, label = "Student's Exam Scores", color = "red", size = 5, hjust = -0.1)
x1_student = 68
x2_student = 75
student_prediction = inv.logit(current_theta0 + current_theta1*x1_student + current_theta2*x2_student)
student_prediction
setwd("~/OneDrive - Emory/NEW Emory University/Emory University/Thesis")
setwd("~/OneDrive - Emory/NEW Emory University/Emory University/DATA 555/Final Project")
